{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/argennof/Data_A-V_2022/blob/main/Lab_4_Object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98rds-2OU-Rd"
      },
      "source": [
        "# ¿Que es Object Detection?\n",
        "\n",
        "Object detection (también llamado a veces *object recognition*) es un término general que se usa para describir una colección de tareas de computer vision que implican identificar **objetos semánticos de una cierta clase** (como humanos, edificios, perros o coches) en imágenes o videos digitales.\n",
        "\n",
        "Mientras que la *clasificación imágenes* implica predecir la clase de un objeto en una imagen (lo que ya hicimos en los labs 1 y 2), la *localización de objetos* implica identificar la ubicación de uno o más objetos en una imagen definiendo un *bounding box* (recuadro delimitador) alrededor de su extensión. La **detección de objetos** combina estas dos tareas para localizar y clasificar uno o más objetos en una imagen, tal como muestra la siguiente imagen.\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/3/38/Detected-with-YOLO--Schreibtisch-mit-Objekten.jpg)\n",
        "\n",
        "Ahora vamos a distinguir estas tres tareas de computer vision.\n",
        "\n",
        "**Image classification**: predice el tipo o clase de un objeto en una imagen.\n",
        "- *Entrada*: una imagen con un solo objeto.\n",
        "- *Salida*: una etiqueta de clase.\n",
        "\n",
        "**Object localization**: localiza la presencia de objetos en una imagen e indique su ubicación con un bounding box.\n",
        "- *Entrada*: una imagen con uno o más objetos.\n",
        "- *Salida*: una o más casillas delimitadoras (por ejemplo, definidas por un punto, ancho y altura).\n",
        "\n",
        "**Object detection**: localiza la presencia de objetos con un bounding box y su respectiva clase.\n",
        "- *Entrada*: una imagen con uno o más objetos.\n",
        "- *Salida*: una o más bounding box (definidas por un punto, ancho y altura), y una etiqueta de clase para cada uno.\n",
        "\n",
        "En la siguiente imagen se muestran diferentes tareas de computer vision y los resultados que producen.\n",
        "\n",
        "![](https://nanonets.com/blog/content/images/size/w1000/2020/08/59b6d0529299e.png)\n",
        "\n",
        "Ahora que ya manejamos una terminología básica vamos a pasar a ver como podemos implementar la tarea de detección de objetos con deep learning en la práctica, usando Python y TensorFlow.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As-4UhRJjA2s"
      },
      "source": [
        "---\n",
        "\n",
        "**NOTA**: NOTA: Este notebook ha sido creado a partir de recursos disponibles en la web que se listan a continuación. La fuente de las imágenes originales se puede consultar directamente el markdown insertado en la celda de texto. \n",
        "\n",
        "- https://machinelearningmastery.com/object-recognition-with-deep-learning/\n",
        "- https://en.wikipedia.org/wiki/Object_detection\n",
        "- https://www.tensorflow.org/hub/tutorials/tf2_object_detection?hl=en\n",
        "- https://www.tensorflow.org/lite/models/object_detection/overview?hl=en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOvvWAVTkMR7"
      },
      "source": [
        "# Object Detection con TensorFlow Hub\n",
        "\n",
        "[TensorFlow Hub (TF Hub)](https://tfhub.dev/) es un repositorio de modelos de aprendizaje automático previamente entrenados, listos para ser optimizados e implementarlos donde quieras con solo unas pocas líneas de código. Para saber más de TF Hub pueden ir a las [guías oficiales](https://www.tensorflow.org/hub/), o chequear el [siguiente tutorial](https://medium.com/ymedialabs-innovation/how-to-use-tensorflow-hub-with-code-examples-9100edec29af).\n",
        "\n",
        "En resumen, TF Hub pone a dispocisión una gran colección de modelos *out-of-the-box* para detección de objetos, cuyos detalles se pueden encontrar en [este link](https://tfhub.dev/s?module-type=image-object-detection). En este notebook vamos a hacer uso de estos modelos, como también funciones provistas por la [API de TF Hub](https://www.tensorflow.org/hub/api_docs/python/hub). \n",
        "\n",
        "En este tutorial vamos revisar los pasos para ejecutar diferentes modelos de detección disponibles en TF Hub sobre imágenes del dataset [COCO 2017](https://cocodataset.org/#explore).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPs64QA1Zdov"
      },
      "source": [
        "## Imports y configuraciones iniciales\n",
        "\n",
        "Como siempre, comenzamos con los imports iniciales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn5_uV1HLvaz"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from six.moves.urllib.request import urlopen\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Keras version\", keras.__version__)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IogyryF2lFBL"
      },
      "source": [
        "## Utilidades\n",
        "\n",
        "La siguiente celda de código crea algunas utilidades que se necesitarán más adelante:\n",
        "\n",
        "- Cargar una imagen.\n",
        "- Administrar los modelos en TF Hub.\n",
        "- Una lista de imágenes para propósitos de testeo (se pueden agregar todas las que se quiera).\n",
        "- Información extra del dataset [COCO 2017](https://cocodataset.org/#home) (necesario para modelos algunos modelos disponibles en TF Hub)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y9R0Xllefec"
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: the file path to the image\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  image = None\n",
        "  if(path.startswith('http')):\n",
        "    response = urlopen(path)\n",
        "    image_data = response.read()\n",
        "    image_data = BytesIO(image_data)\n",
        "    image = Image.open(image_data)\n",
        "  else:\n",
        "    image_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "    image = Image.open(BytesIO(image_data))\n",
        "\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (1, im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "ALL_MODELS = {\n",
        "'CenterNet HourGlass104 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512/1',\n",
        "'CenterNet HourGlass104 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1',\n",
        "'CenterNet HourGlass104 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024/1',\n",
        "'CenterNet HourGlass104 Keypoints 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024_kpts/1',\n",
        "'CenterNet Resnet50 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1',\n",
        "'CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512_kpts/1',\n",
        "'CenterNet Resnet101 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1',\n",
        "'CenterNet Resnet50 V2 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512/1',\n",
        "'CenterNet Resnet50 V2 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512_kpts/1',\n",
        "'EfficientDet D0 512x512' : 'https://tfhub.dev/tensorflow/efficientdet/d0/1',\n",
        "'EfficientDet D1 640x640' : 'https://tfhub.dev/tensorflow/efficientdet/d1/1',\n",
        "'EfficientDet D2 768x768' : 'https://tfhub.dev/tensorflow/efficientdet/d2/1',\n",
        "'EfficientDet D3 896x896' : 'https://tfhub.dev/tensorflow/efficientdet/d3/1',\n",
        "'EfficientDet D4 1024x1024' : 'https://tfhub.dev/tensorflow/efficientdet/d4/1',\n",
        "'EfficientDet D5 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d5/1',\n",
        "'EfficientDet D6 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d6/1',\n",
        "'EfficientDet D7 1536x1536' : 'https://tfhub.dev/tensorflow/efficientdet/d7/1',\n",
        "'SSD MobileNet v2 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2',\n",
        "'SSD MobileNet V1 FPN 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v1/fpn_640x640/1',\n",
        "'SSD MobileNet V2 FPNLite 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1',\n",
        "'SSD MobileNet V2 FPNLite 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1',\n",
        "'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1',\n",
        "'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_1024x1024/1',\n",
        "'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1',\n",
        "'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_1024x1024/1',\n",
        "'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_640x640/1',\n",
        "'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_1024x1024/1',\n",
        "'Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',\n",
        "'Faster R-CNN ResNet50 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet50 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1',\n",
        "'Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',\n",
        "'Faster R-CNN ResNet101 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet101 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1',\n",
        "'Faster R-CNN ResNet152 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1',\n",
        "'Faster R-CNN ResNet152 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet152 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_800x1333/1',\n",
        "'Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',\n",
        "'Faster R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1',\n",
        "'Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1',\n",
        "}\n",
        "\n",
        "IMAGES_FOR_TEST = {\n",
        "  'Beach' : 'models/research/object_detection/test_images/image2.jpg',\n",
        "  'Dogs' : 'models/research/object_detection/test_images/image1.jpg',\n",
        "  # By Heiko Gorski, Source: https://commons.wikimedia.org/wiki/File:Naxos_Taverna.jpg\n",
        "  'Naxos Taverna' : 'https://upload.wikimedia.org/wikipedia/commons/6/60/Naxos_Taverna.jpg',\n",
        "  # Source: https://commons.wikimedia.org/wiki/File:The_Coleoptera_of_the_British_islands_(Plate_125)_(8592917784).jpg\n",
        "  'Beatles' : 'https://upload.wikimedia.org/wikipedia/commons/1/1b/The_Coleoptera_of_the_British_islands_%28Plate_125%29_%288592917784%29.jpg',\n",
        "  # By Américo Toledano, Source: https://commons.wikimedia.org/wiki/File:Biblioteca_Maim%C3%B3nides,_Campus_Universitario_de_Rabanales_007.jpg\n",
        "  'Phones' : 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg/1024px-Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg',\n",
        "  # Source: https://commons.wikimedia.org/wiki/File:The_smaller_British_birds_(8053836633).jpg\n",
        "  'Birds' : 'https://upload.wikimedia.org/wikipedia/commons/0/09/The_smaller_British_birds_%288053836633%29.jpg',\n",
        "}\n",
        "\n",
        "COCO17_HUMAN_POSE_KEYPOINTS = [(0, 1),\n",
        " (0, 2),\n",
        " (1, 3),\n",
        " (2, 4),\n",
        " (0, 5),\n",
        " (0, 6),\n",
        " (5, 7),\n",
        " (7, 9),\n",
        " (6, 8),\n",
        " (8, 10),\n",
        " (5, 6),\n",
        " (5, 11),\n",
        " (6, 12),\n",
        " (11, 12),\n",
        " (11, 13),\n",
        " (13, 15),\n",
        " (12, 14),\n",
        " (14, 16)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14bNk1gzh0TN"
      },
      "source": [
        "## Herramientas de visualización\n",
        "\n",
        "Para visualizar los objetos detectados con los bounding boxes y su segmentación, utilizaremos la *Object Detection API* de TensorFlow. Para instalarlo clonamos el repositorio el github."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi28cqGGFWnY"
      },
      "source": [
        "# Clone the tensorflow models repository\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX3pb_pXDjYA"
      },
      "source": [
        "Ahora instalamos la Object Detection API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwdsBdGhFanc"
      },
      "source": [
        "%%bash\n",
        "sudo apt install -y protobuf-compiler\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip --use-deprecated=legacy-resolver install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yDNgIx-kV7X"
      },
      "source": [
        "Importamos las librerías que vamos a necesitar en el notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JCeQU3fkayh"
      },
      "source": [
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKtD0IeclbL5"
      },
      "source": [
        "Y finalmente vamos a cargar las etiquetas del dataset. Las etiquetas (o labels) corresponden a números enteros que están asociados a los nombres de las categorías del dataset, de modo que cuando CNN predice `5`, sabemos que esto corresponde a `airplane`. Para esto vamos a usar algunas funciones de la API que nos ayudan a simplificar el código."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mucYUS6exUJ"
      },
      "source": [
        "PATH_TO_LABELS = './models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6917xnUSlp9x"
      },
      "source": [
        "## Eligiendo el modelo de detección\n",
        "\n",
        "Ahora vamos a elegir el modelo de detección de objetos que queramos utilizar. Para esto, tenemos que seleccionar la arquitectura deseada y el modelo se cargará automáticamente. Acá estamos usando los *form* del Colab, por lo que la lista de modelos la pueden elegir de la lista desplegable de la derecha (acá les dejo el [tutorial de Colab](https://colab.research.google.com/drive/1a6y9DN3BNA8qim-1hqCqId48hUoY44Cr) donde se introducen los forms)\n",
        "\n",
        "Si se quiere cambiar el modelo para probar otras arquitecturas más tarde, simplemente se elige otra y se ejecuta la celda y todas las siguientes (recordá que lo podes hacer con un solo clik desde el menu `Entorno de Ejecución -> Ejecutar celda seleccionada y las siguientes`).\n",
        "\n",
        "**NOTA**: si desea conocer más detalles sobre el modelo seleccionado, puede seguir el enlace que se imprime en la salida (también disponible en la lista `ALL_MODELS`) y leer la documentación adicional en TF Hub.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtwrSqvakTNn"
      },
      "source": [
        "#@title Model Selection\n",
        "model_display_name = 'EfficientDet D4 1024x1024' # @param ['CenterNet HourGlass104 512x512','CenterNet HourGlass104 Keypoints 512x512','CenterNet HourGlass104 1024x1024','CenterNet HourGlass104 Keypoints 1024x1024','CenterNet Resnet50 V1 FPN 512x512','CenterNet Resnet50 V1 FPN Keypoints 512x512','CenterNet Resnet101 V1 FPN 512x512','CenterNet Resnet50 V2 512x512','CenterNet Resnet50 V2 Keypoints 512x512','EfficientDet D0 512x512','EfficientDet D1 640x640','EfficientDet D2 768x768','EfficientDet D3 896x896','EfficientDet D4 1024x1024','EfficientDet D5 1280x1280','EfficientDet D6 1280x1280','EfficientDet D7 1536x1536','SSD MobileNet v2 320x320','SSD MobileNet V1 FPN 640x640','SSD MobileNet V2 FPNLite 320x320','SSD MobileNet V2 FPNLite 640x640','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)','SSD ResNet152 V1 FPN 640x640 (RetinaNet152)','SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)','Faster R-CNN ResNet50 V1 640x640','Faster R-CNN ResNet50 V1 1024x1024','Faster R-CNN ResNet50 V1 800x1333','Faster R-CNN ResNet101 V1 640x640','Faster R-CNN ResNet101 V1 1024x1024','Faster R-CNN ResNet101 V1 800x1333','Faster R-CNN ResNet152 V1 640x640','Faster R-CNN ResNet152 V1 1024x1024','Faster R-CNN ResNet152 V1 800x1333','Faster R-CNN Inception ResNet V2 640x640','Faster R-CNN Inception ResNet V2 1024x1024','Mask R-CNN Inception ResNet V2 1024x1024']\n",
        "model_handle = ALL_MODELS[model_display_name]\n",
        "\n",
        "print('Selected model:'+ model_display_name)\n",
        "print('Model Handle at TensorFlow Hub: {}'.format(model_handle))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muhUt-wWL582"
      },
      "source": [
        "## Cargando el modelo seleccionado\n",
        "\n",
        "Acá solo necesitamos el identificador del modelo que se seleccionó y usamos la biblioteca de Tensorflow Hub para cargarlo en la memoria. \n",
        "\n",
        "**Este proceso lleva algunos minutos** (les sobre para calentar el agua del mate o hacerse un café).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBuD07fLlcEO"
      },
      "source": [
        "print('loading model...')\n",
        "hub_model = hub.load(model_handle)\n",
        "print('model loaded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx7Xopat-CAe"
      },
      "source": [
        "**Opcionalmente**, se pueden cargar los modelos disponibles en TF Hub para ser usados con la libreria Keras (tal como veníamos haciendo en los notebooks anteriores). Una ventaja de esto es que podemos visualizar la arquitectura de la red mediante la función `summary()`. Dado que la siguiente celda requiere varios minutos para ejecutarse comentamos su contenido, pero si les da curiosidad mirar de cerca la arquitectura del modelo elegido no dejen de ejecutarla! (aunque seguramente requiera algo más de esfuerzo para acomodar las capas de input y output del modelo keras).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOJu89lh7HKs"
      },
      "source": [
        "#print('loading keras model...')\n",
        "#keras_model = tf.keras.Sequential([\n",
        "#              hub.KerasLayer(model_handle,\n",
        "#                              output_shape=[20],\n",
        "#                              input_shape=[]),\n",
        "#              tf.keras.layers.Dense(10,\n",
        "#                              activation='softmax')])\n",
        "#print('model loaded!')\n",
        "#\n",
        "#keras_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIawRDKPPnd4"
      },
      "source": [
        "## Cargando una imagen de testeo\n",
        "\n",
        "Dado que todos los modelos en la colección TensorFlow Hud ya han sido entrenados, podemos comenzar a clasificar nuevas imágenes son hacer nada más. Ahora probemos el modelo en una imagen simple. Para ayudar con esto vamos a usar la lista de imágenes de prueba disponible en la primera celda de código del notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX-AWUQ1wIEr"
      },
      "source": [
        "#@title Image Selection (NO TE OLVIDES DE EJECUTAR LA CELDA!) \n",
        "selected_image = 'Beach' #@param ['Beach', 'Dogs', 'Naxos Taverna', 'Beatles', 'Phones', 'Birds']\n",
        "flip_image_horizontally = True #@param {type:\"boolean\"}\n",
        "convert_image_to_grayscale = True #@param {type:\"boolean\"}\n",
        "\n",
        "image_path = IMAGES_FOR_TEST[selected_image]\n",
        "image_np = load_image_into_numpy_array(image_path)\n",
        "\n",
        "# Flip horizontally\n",
        "if(flip_image_horizontally):\n",
        "  image_np[0] = np.fliplr(image_np[0]).copy()\n",
        "\n",
        "# Convert image to grayscale\n",
        "if(convert_image_to_grayscale):\n",
        "  image_np[0] = np.tile(\n",
        "    np.mean(image_np[0], 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
        "\n",
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(image_np[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTHsFjR6HNwb"
      },
      "source": [
        "## Detectando objetos\n",
        "\n",
        "Para hacer la inferencia solo necesitamos llamar a nuestro modelo TF Hub recien cargado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb_siXKcnnGC"
      },
      "source": [
        "# running inference\n",
        "results = hub_model(image_np)\n",
        "\n",
        "# different object detection models have additional results\n",
        "# all of them are explained in the documentation\n",
        "result = {key:value.numpy() for key,value in results.items()}\n",
        "print(result.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ5VYaBoeeFM"
      },
      "source": [
        "## Visualizando los resultados\n",
        "\n",
        "Acá vamos a utilizar la API de detección de objetos de TensorFlow para mostrar los bounding box de las detecciones. \n",
        "\n",
        "**TIP**: en la función `visualize_boxes_and_labels_on_image_array()` (linea 15) puede establecer valores diferentes para `min_score_thresh` y `max_boxes_to_draw` para permitir más o menos detecciones en la imagen de salida. La documentación completa de este método se puede ver [acá](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O7rV8g9s8Bz"
      },
      "source": [
        "#@title Visualizar los resultados \n",
        "\n",
        "label_id_offset = 0 \n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "max_boxes_to_draw=200 #@param {type:\"slider\", min:1, max:300, step:1}\n",
        "min_score_thresh=0.3 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "# Use keypoints if available in detections\n",
        "keypoints, keypoint_scores = None, None\n",
        "if 'detection_keypoints' in result:\n",
        "  keypoints = result['detection_keypoints'][0]\n",
        "  keypoint_scores = result['detection_keypoint_scores'][0]\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_detections[0],\n",
        "      result['detection_boxes'][0],\n",
        "      (result['detection_classes'][0] + label_id_offset).astype(int),\n",
        "      result['detection_scores'][0],\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      max_boxes_to_draw=max_boxes_to_draw,\n",
        "      min_score_thresh=min_score_thresh,\n",
        "      agnostic_mode=False,\n",
        "      keypoints=keypoints,\n",
        "      keypoint_scores=keypoint_scores,\n",
        "      keypoint_edges=COCO17_HUMAN_POSE_KEYPOINTS)\n",
        "\n",
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(image_np_with_detections[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ln55VWBfjgc"
      },
      "source": [
        "---\n",
        "\n",
        "# Trabajo Práctico 2 (primera parte)\n",
        "\n",
        "**Acá tienen que dejar los datos de las y los integrantes del grupo:**\n",
        "\n",
        "Nombre y Apellido, DNI, correo eletrónico\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j3DS0bPOXJd"
      },
      "source": [
        "## **EJERCICIO 4.1**\n",
        "\n",
        "Ahora vamos a probar algunas varianes simples sobre el tutorial previamente desarrollado. La consigna de este ejercicio es redactar un pequeño informe (celdas de texto en este mismo notebook) sobre los resutlados obtenidos.\n",
        "\n",
        "1. **Elejir el modelo de detección**. Indague en la web sobre los algoritmos del estado de arte para detección de objetos.\n",
        " - ¿Encontrás alguno de estos algoritmos en la lista de modelos disponible en TF Hub? Reportá al menos 3.\n",
        " - Seleccioná uno de los tres modelos justificando por que se eligió.\n",
        " - Volvé a ejecutar el tutorial para ese modelo\n",
        "\n",
        "2. **Modificar las imágenes de entrada**. Con este nuevo modelo, volvé a detectar objetos en las imágenes de testeo, pero ahora activá y desactivá los checkbox de la celda donde cargamos las imágenes de prueba. \n",
        " - ¿Notas diferencias en los resultados de detección entre la imagen original y la transformada? Informá cuales son estas diferencias (incluso podés mostrar imágenes con los diferentes resutaldos).\n",
        "\n",
        "3. **Ejecutar la detección en tus propias imágenes**. Ya comentamos que podés agregar todas las imágenes que quieras en la celda de selección de imágenes testeo. Buscá en la web al menos 2 imágenes de cada una de las siguientes categorías: animales marinos, figuras prediseñadas (dibujos digitales en color o blanco y negro), edificios, monumentos (pirámides, torre Eiffel, Obelisco, etc.), montañas (sin personas, sin objetos hechos por el hombre, etc.), bosques.\n",
        " - ¿Que tan bien funciona detectando objetos de estas categorias?\n",
        " - ¿Cuál crees que sea el problema de estos modelos para detectar estos objetos? (TIP: podes revisar las características del [dataset COCO 2017](https://cocodataset.org/#explore))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY99Fi7WTwDb"
      },
      "source": [
        "### INFORME EJERCICIO 1\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qaw6Xi08NpEP"
      },
      "source": [
        "## **EJERCICIO 4.2** (Opcional)\n",
        "\n",
        "Entre los modelos de detección de objetos disponibles se encuentra *Mask R-CNN*. Lo interesante de este modelo es que permite obtener como resultado la máscara de segmentación de instancias.\n",
        "\n",
        "**CUIDADO**: antes de continuar tenes que ir a la celda de selección de modelos, elegir *Mask R-CNN* y volver a ejecutar esa celda y todas las siguientes (CTRL+F10 desde la celda de selección de modelos)\n",
        "\n",
        "Para visualizarlo usaremos el mismo método que usamos antes pero agregando un parámetro adicional: `instance_masks=output_dict.get('detection_masks_reframed', None)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl3qdtR1OvM_"
      },
      "source": [
        "max_boxes_to_draw=200 #@param {type:\"slider\", min:1, max:300, step:1}\n",
        "min_score_thresh=0.3 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "# Handle models with masks:\n",
        "image_np_with_mask = image_np.copy()\n",
        "\n",
        "if 'detection_masks' in result:\n",
        "  # we need to convert np.arrays to tensors\n",
        "  detection_masks = tf.convert_to_tensor(result['detection_masks'][0])\n",
        "  detection_boxes = tf.convert_to_tensor(result['detection_boxes'][0])\n",
        "\n",
        "  # Reframe the the bbox mask to the image size.\n",
        "  detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "            detection_masks, detection_boxes,\n",
        "              image_np.shape[1], image_np.shape[2])\n",
        "  detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
        "                                      tf.uint8)\n",
        "  result['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_mask[0],\n",
        "      result['detection_boxes'][0],\n",
        "      (result['detection_classes'][0] + label_id_offset).astype(int),\n",
        "      result['detection_scores'][0],\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      max_boxes_to_draw=max_boxes_to_draw,\n",
        "      min_score_thresh=min_score_thresh,\n",
        "      agnostic_mode=False,\n",
        "      instance_masks=result.get('detection_masks_reframed', None),\n",
        "      line_thickness=8)\n",
        "\n",
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(image_np_with_mask[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIx0IAhnN1se"
      },
      "source": [
        "Ahora vamos a visualizar solamente la máscara. Para esto, tenemos que indicar en `detection_masks_reframed` el índice que corresponde a la máscara del objeto que queremos mostrar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvp99jegNn2v"
      },
      "source": [
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(detection_masks_reframed[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww4fSsK2MGQT"
      },
      "source": [
        "Con todo esto, ahora tenemos los primeros elementos para poder evaluar que tan bien funciona el detector de objetos sobre una imágenes en particular. Para esto deben indagar sobre las medidas de rendimiento típicamente utilizadas en la detección de objetos (TIP: buscar IoU y Dice). En la web existen muchos ejemplos de como utilizar estas medidas, incluso hay código disponible. \n",
        "\n",
        "A grandes rasgos, la idea de estas medidas es realizar una comparación de la máscara producida por el algoritmo (generalmente nombrada **prediction**) con una máscara hecha a mano (generalmente llamada **groundtruth**). Por lo tanto, el groundtruth es una imagen similar a la que se muestra arriba, pero donde la segmentación la hace una persona. A continuación se deja un ejemplo más concreto, donde se suporpone la imagen original con el groundtruth (izquierda) y la predicción del algoritmo (derecha).\n",
        "\n",
        "![](https://www.jeremyjordan.me/content/images/2018/05/target_prediction.png)\n",
        "\n",
        "El dataset COCO 2017 ofrece máscaras para muchas de sus imágenes.\n",
        "\n",
        "Por lo tanto, la consigna de este ejercicio es la siguiente:\n",
        "\n",
        "1. Evalue el rendimiento del modelo sobre un conjunto de imágenes de prueba (TIP: indague métricas para instance segmentation y reutilice código disponible en la web)\n",
        "\n",
        "2. Reporte los resultados obtenidos"
      ]
    }
  ]
}